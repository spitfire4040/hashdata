{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33fa96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ef9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# url = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/hashdata2.csv\"\n",
    "url = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/10-minute-hashes.csv\"\n",
    "# url = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/9-minute-hashes.csv\"\n",
    "# url = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/8-minute-hashes.csv\"\n",
    "names = ['dim1','dim2','dim3','dim4','dim5','dim6','dim7','dim8','dim9','dim10','dim11',\n",
    "         'dim12','dim13','dim14','dim15','dim16','dim17','dim18','dim19','dim20','dim21',\n",
    "         'dim22','dim23','dim24','dim25','dim26','dim27','dim28','dim29','dim30','dim31','dim32','class']\n",
    "dataset = read_csv(url, names=names)\n",
    "\n",
    "x = dataset.drop(['class'], axis=1)\n",
    "print(len(x))\n",
    "y_original = dataset['class'].values.tolist()\n",
    "print(dataset['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset['class']\n",
    "print(y.unique())\n",
    "\n",
    "counter = 0\n",
    "\n",
    "y_temp = dataset['class'].tolist()\n",
    "\n",
    "for unique_value in sorted(y.unique()):\n",
    "    for index, value in enumerate(y):\n",
    "        if value == unique_value:\n",
    "            y_temp[index] = counter\n",
    "    counter += 1\n",
    "\n",
    "dataset[\"class\"] = y_temp\n",
    "y = dataset['class']\n",
    "print(y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c1fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = dataset['class']\n",
    "# print(sorted(y.unique()))\n",
    "# print(len(y.unique()))\n",
    "# counter = 0\n",
    "# for unique_value in sorted(y.unique()):\n",
    "#     for index, value in enumerate(y):\n",
    "#         if value is unique_value:\n",
    "#             y.at[index] = counter\n",
    "#     counter += 1\n",
    "# print(y.unique())\n",
    "# print(len(y.unique()))\n",
    "# print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# url = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/hashdata2.csv\"\n",
    "url_10_minute = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/10-minute-hashes.csv\"\n",
    "names = ['dim1','dim2','dim3','dim4','dim5','dim6','dim7','dim8','dim9','dim10','dim11',\n",
    "         'dim12','dim13','dim14','dim15','dim16','dim17','dim18','dim19','dim20','dim21',\n",
    "         'dim22','dim23','dim24','dim25','dim26','dim27','dim28','dim29','dim30','dim31','dim32','class']\n",
    "dataset_10_minute = read_csv(url_10_minute, names=names)\n",
    "\n",
    "x_10_minute = dataset_10_minute.drop(['class'], axis=1)\n",
    "print(len(x_10_minute))\n",
    "y_original_10_minute = dataset_10_minute['class'].values.tolist()\n",
    "print(dataset_10_minute['class'].unique())\n",
    "\n",
    "y_10_minute = dataset_10_minute['class']\n",
    "print(y_10_minute.unique())\n",
    "\n",
    "counter_10_minute = 0\n",
    "\n",
    "y_temp_10_minute = dataset_10_minute['class'].tolist()\n",
    "\n",
    "for unique_value in sorted(y_10_minute.unique()):\n",
    "    for index, value in enumerate(y_10_minute):\n",
    "        if value == unique_value:\n",
    "            y_temp_10_minute[index] = counter_10_minute\n",
    "    counter_10_minute += 1\n",
    "\n",
    "dataset_10_minute[\"class\"] = y_temp_10_minute\n",
    "y_10_minute = dataset_10_minute['class']\n",
    "print(y_10_minute.unique())\n",
    "\n",
    "x_train_10_minute, x_test_10_minute, y_train_10_minute, y_test_10_minute = train_test_split(\n",
    "    x_10_minute.values.tolist(), \n",
    "    y_10_minute.values.tolist(), \n",
    "    test_size=0.30)\n",
    "\n",
    "# Spot Check Algorithms\n",
    "models_10_minute = []\n",
    "models_10_minute.append(('LR', LogisticRegression(n_jobs=12)))\n",
    "models_10_minute.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models_10_minute.append(('KNN', KNeighborsClassifier(n_jobs=12)))\n",
    "models_10_minute.append(('CART', DecisionTreeClassifier()))\n",
    "models_10_minute.append(('NB', GaussianNB()))\n",
    "models_10_minute.append(('SVM', SVC(gamma='auto', max_iter=1000)))\n",
    "# models_10_minute.append(('nuSVM', NuSVC(max_iter=1000)))\n",
    "models_10_minute.append(('linearSVM', LinearSVC(max_iter=1000)))\n",
    "models_10_minute.append(('SGD', SGDClassifier(n_jobs=12)))\n",
    "models_10_minute.append(('MLP', MLPClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results_10_minute = []\n",
    "names_10_minute = []\n",
    "for name, model in models_10_minute:\n",
    "    print(name, model)\n",
    "    model.fit(x_train_10_minute, y_train_10_minute)\n",
    "    y_pred = model.predict(x_test_10_minute)\n",
    "    results_10_minute.append(accuracy_score(y_test_10_minute, y_pred))\n",
    "    names_10_minute.append(name)\n",
    "    \n",
    "    result_mean = 0\n",
    "    for result in results_10_minute:\n",
    "        result_mean += result\n",
    "    result_mean /= len(results_10_minute)\n",
    "    \n",
    "    print(f'{name}, {result_mean}')\n",
    "    \n",
    "# evaluate each model in turn\n",
    "results_k_fold_10_minute = []\n",
    "names_k_fold_10_minute = []\n",
    "for name, model in models_10_minute:\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    cv_results = cross_val_score(model, x_train_10_minute, y_train_10_minute, cv=kfold, scoring='accuracy')\n",
    "    results_k_fold_10_minute.append(cv_results)\n",
    "    names_k_fold_10_minute.append(name)\n",
    "    print(f'{name}: {cv_results.mean()} ({cv_results.std()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29962a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66789\n",
      "['Tenvis_Cam' 'Minger_LightStrip' 'Wans_Cam' 'tp-link_SmartPlug'\n",
      " 'LaCrosse_AlarmClock' 'Lumiman_Bulb900' 'Lumiman_Bulb600' 'Smart_Lamp'\n",
      " 'Renpho_SmartPlug' 'Gosuna_LightBulb' 'Goumia_Coffemaker'\n",
      " 'oossxx_SmartPlug' 'Ring_Doorbell' 'Ocean_Radio' 'Gosuna_Socket'\n",
      " 'Chime_Doorbell' 'Smart_LightStrip' 'Wemo_SmartPlug' 'D-Link_Cam936L'\n",
      " 'tp-link_LightBulb' 'Lumiman_SmartPlug' 'itTiot_Cam']\n",
      "['Tenvis_Cam' 'Minger_LightStrip' 'Wans_Cam' 'tp-link_SmartPlug'\n",
      " 'LaCrosse_AlarmClock' 'Lumiman_Bulb900' 'Lumiman_Bulb600' 'Smart_Lamp'\n",
      " 'Renpho_SmartPlug' 'Gosuna_LightBulb' 'Goumia_Coffemaker'\n",
      " 'oossxx_SmartPlug' 'Ring_Doorbell' 'Ocean_Radio' 'Gosuna_Socket'\n",
      " 'Chime_Doorbell' 'Smart_LightStrip' 'Wemo_SmartPlug' 'D-Link_Cam936L'\n",
      " 'tp-link_LightBulb' 'Lumiman_SmartPlug' 'itTiot_Cam']\n",
      "[15  9 16 21  5  7  6 13 11  2  4 19 12 10  3  0 14 17  1 20  8 18]\n",
      "LR LogisticRegression(n_jobs=12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, 0.9502420522034236\n",
      "LDA LinearDiscriminantAnalysis()\n",
      "LDA, 0.9392374107900384\n",
      "KNN KNeighborsClassifier(n_jobs=12)\n",
      "KNN, 0.9447854801949727\n",
      "CART DecisionTreeClassifier()\n",
      "CART, 0.9460622847731697\n",
      "NB GaussianNB()\n",
      "NB, 0.9423865848180866\n",
      "SVM SVC(gamma='auto', max_iter=1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/svm/_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, 0.9171948561827286\n",
      "linearSVM LinearSVC()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linearSVM, 0.9208820824332128\n",
      "SGD SGDClassifier(n_jobs=12)\n",
      "SGD, 0.9238284174277586\n",
      "MLP MLPClassifier()\n",
      "MLP, 0.9272956142247951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.9526222650192357 (0.002389586712766591)\n",
      "LDA: 0.9305913139343925 (0.0023520044133248964)\n",
      "KNN: 0.9578199338526918 (0.001438890736989578)\n",
      "CART: 0.9526008975174174 (0.0019716470288754554)\n",
      "NB: 0.9219070186593964 (0.003562021964495076)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/svm/_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/svm/_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/svm/_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/svm/_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/svm/_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/svm/_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "/home/nthom/anaconda3/envs/hash/lib/python3.9/site-packages/sklearn/svm/_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "# url = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/hashdata2.csv\"\n",
    "url_9_minute = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/9-minute-hashes.csv\"\n",
    "names = ['dim1','dim2','dim3','dim4','dim5','dim6','dim7','dim8','dim9','dim10','dim11',\n",
    "         'dim12','dim13','dim14','dim15','dim16','dim17','dim18','dim19','dim20','dim21',\n",
    "         'dim22','dim23','dim24','dim25','dim26','dim27','dim28','dim29','dim30','dim31','dim32','class']\n",
    "dataset_9_minute = read_csv(url_9_minute, names=names)\n",
    "\n",
    "x_9_minute = dataset_9_minute.drop(['class'], axis=1)\n",
    "print(len(x_9_minute))\n",
    "y_original_9_minute = dataset_9_minute['class'].values.tolist()\n",
    "print(dataset_9_minute['class'].unique())\n",
    "\n",
    "y_9_minute = dataset_9_minute['class']\n",
    "print(y_9_minute.unique())\n",
    "\n",
    "counter_9_minute = 0\n",
    "\n",
    "y_temp_9_minute = dataset_9_minute['class'].tolist()\n",
    "\n",
    "for unique_value in sorted(y_9_minute.unique()):\n",
    "    for index, value in enumerate(y_9_minute):\n",
    "        if value == unique_value:\n",
    "            y_temp_9_minute[index] = counter_9_minute\n",
    "    counter_9_minute += 1\n",
    "\n",
    "dataset_9_minute[\"class\"] = y_temp_9_minute\n",
    "y_9_minute = dataset_9_minute['class']\n",
    "print(y_9_minute.unique())\n",
    "\n",
    "x_train_9_minute, x_test_9_minute, y_train_9_minute, y_test_9_minute = train_test_split(\n",
    "    x_9_minute.values.tolist(), \n",
    "    y_9_minute.values.tolist(), \n",
    "    test_size=0.30)\n",
    "\n",
    "# Spot Check Algorithms\n",
    "models_9_minute = []\n",
    "models_9_minute.append(('LR', LogisticRegression(n_jobs=12)))\n",
    "models_9_minute.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models_9_minute.append(('KNN', KNeighborsClassifier(n_jobs=12)))\n",
    "models_9_minute.append(('CART', DecisionTreeClassifier()))\n",
    "models_9_minute.append(('NB', GaussianNB()))\n",
    "models_9_minute.append(('SVM', SVC(gamma='auto', max_iter=1000)))\n",
    "# models_9_minute.append(('nuSVM', NuSVC(max_iter=1000)))\n",
    "models_9_minute.append(('linearSVM', LinearSVC(max_iter=1000)))\n",
    "models_9_minute.append(('SGD', SGDClassifier(n_jobs=12)))\n",
    "models_9_minute.append(('MLP', MLPClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results_9_minute = []\n",
    "names_9_minute = []\n",
    "for name, model in models_9_minute:\n",
    "    print(name, model)\n",
    "    model.fit(x_train_9_minute, y_train_9_minute)\n",
    "    y_pred = model.predict(x_test_9_minute)\n",
    "    results_9_minute.append(accuracy_score(y_test_9_minute, y_pred))\n",
    "    names_9_minute.append(name)\n",
    "    \n",
    "    result_mean = 0\n",
    "    for result in results_9_minute:\n",
    "        result_mean += result\n",
    "    result_mean /= len(results_9_minute)\n",
    "    \n",
    "    print(f'{name}, {result_mean}')\n",
    "    \n",
    "# evaluate each model in turn\n",
    "results_k_fold_9_minute = []\n",
    "names_k_fold_9_minute = []\n",
    "for name, model in models_9_minute:\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    cv_results = cross_val_score(model, x_train_9_minute, y_train_9_minute, cv=kfold, scoring='accuracy')\n",
    "    results_k_fold_9_minute.append(cv_results)\n",
    "    names_k_fold_9_minute.append(name)\n",
    "    print(f'{name}: {cv_results.mean()} ({cv_results.std()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f02dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# url = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/hashdata2.csv\"\n",
    "url_8_minute = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/9-minute-hashes.csv\"\n",
    "names = ['dim1','dim2','dim3','dim4','dim5','dim6','dim7','dim8','dim9','dim10','dim11',\n",
    "         'dim12','dim13','dim14','dim15','dim16','dim17','dim18','dim19','dim20','dim21',\n",
    "         'dim22','dim23','dim24','dim25','dim26','dim27','dim28','dim29','dim30','dim31','dim32','class']\n",
    "dataset_8_minute = read_csv(url_8_minute, names=names)\n",
    "\n",
    "x_8_minute = dataset_8_minute.drop(['class'], axis=1)\n",
    "print(len(x_8_minute))\n",
    "y_original_8_minute = dataset_8_minute['class'].values.tolist()\n",
    "print(dataset_8_minute['class'].unique())\n",
    "\n",
    "y_8_minute = dataset_8_minute['class']\n",
    "print(y_8_minute.unique())\n",
    "\n",
    "counter_8_minute = 0\n",
    "\n",
    "y_temp_8_minute = dataset_8_minute['class'].tolist()\n",
    "\n",
    "for unique_value in sorted(y_8_minute.unique()):\n",
    "    for index, value in enumerate(y_8_minute):\n",
    "        if value == unique_value:\n",
    "            y_temp_8_minute[index] = counter_8_minute\n",
    "    counter_8_minute += 1\n",
    "\n",
    "dataset_8_minute[\"class\"] = y_temp_8_minute\n",
    "y_8_minute = dataset_8_minute['class']\n",
    "print(y_8_minute.unique())\n",
    "\n",
    "x_train_8_minute, x_test_8_minute, y_train_8_minute, y_test_8_minute = train_test_split(\n",
    "    x_8_minute.values.tolist(), \n",
    "    y_8_minute.values.tolist(), \n",
    "    test_size=0.30)\n",
    "\n",
    "# Spot Check Algorithms\n",
    "models_8_minute = []\n",
    "models_8_minute.append(('LR', LogisticRegression(n_jobs=12)))\n",
    "models_8_minute.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models_8_minute.append(('KNN', KNeighborsClassifier(n_jobs=12)))\n",
    "models_8_minute.append(('CART', DecisionTreeClassifier()))\n",
    "models_8_minute.append(('NB', GaussianNB()))\n",
    "models_8_minute.append(('SVM', SVC(gamma='auto', max_iter=1000)))\n",
    "# models_8_minute.append(('nuSVM', NuSVC(max_iter=1000)))\n",
    "models_8_minute.append(('linearSVM', LinearSVC(max_iter=1000)))\n",
    "models_8_minute.append(('SGD', SGDClassifier(n_jobs=12)))\n",
    "models_8_minute.append(('MLP', MLPClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results_8_minute = []\n",
    "names_8_minute = []\n",
    "for name, model in models_8_minute:\n",
    "    print(name, model)\n",
    "    model.fit(x_train_8_minute, y_train_8_minute)\n",
    "    y_pred = model.predict(x_test_8_minute)\n",
    "    results_8_minute.append(accuracy_score(y_test_8_minute, y_pred))\n",
    "    names_8_minute.append(name)\n",
    "    \n",
    "    result_mean = 0\n",
    "    for result in results_8_minute:\n",
    "        result_mean += result\n",
    "    result_mean /= len(results_8_minute)\n",
    "    \n",
    "    print(f'{name}, {result_mean}')\n",
    "    \n",
    "# evaluate each model in turn\n",
    "results_k_fold_8_minute = []\n",
    "names_k_fold_8_minute = []\n",
    "for name, model in models_8_minute:\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    cv_results = cross_val_score(model, x_train_8_minute, y_train_8_minute, cv=kfold, scoring='accuracy')\n",
    "    results_k_fold_8_minute.append(cv_results)\n",
    "    names_k_fold_8_minute.append(name)\n",
    "    print(f'{name}: {cv_results.mean()} ({cv_results.std()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# url = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/hashdata2.csv\"\n",
    "url = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/10-minute-hashes.csv\"\n",
    "# url = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/9-minute-hashes.csv\"\n",
    "# url = \"https://raw.githubusercontent.com/spitfire4040/hashdata/main/8-minute-hashes.csv\"\n",
    "names = ['dim1','dim2','dim3','dim4','dim5','dim6','dim7','dim8','dim9','dim10','dim11',\n",
    "         'dim12','dim13','dim14','dim15','dim16','dim17','dim18','dim19','dim20','dim21',\n",
    "         'dim22','dim23','dim24','dim25','dim26','dim27','dim28','dim29','dim30','dim31','dim32','class']\n",
    "dataset = read_csv(url, names=names)\n",
    "\n",
    "x = dataset.drop(['class'], axis=1)\n",
    "print(len(x))\n",
    "y_original = dataset['class'].values.tolist()\n",
    "print(dataset['class'].unique())\n",
    "\n",
    "y = dataset['class']\n",
    "print(y.unique())\n",
    "\n",
    "counter = 0\n",
    "\n",
    "y_temp = dataset['class'].tolist()\n",
    "\n",
    "for unique_value in sorted(y.unique()):\n",
    "    for index, value in enumerate(y):\n",
    "        if value == unique_value:\n",
    "            y_temp[index] = counter\n",
    "    counter += 1\n",
    "\n",
    "dataset[\"class\"] = y_temp\n",
    "y = dataset['class']\n",
    "print(y.unique())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x.values.tolist(), y.values.tolist(), test_size=0.30)\n",
    "\n",
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(n_jobs=12)))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier(n_jobs=12)))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto', max_iter=1000)))\n",
    "models.append(('nuSVM', NuSVC(max_iter=1000)))\n",
    "models.append(('linearSVM', LinearSVC(max_iter=1000)))\n",
    "models.append(('SGD', SGDClassifier(n_jobs=12)))\n",
    "models.append(('MLP', MLPClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    print(name, model)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    results.append(accuracy_score(y_test, y_pred))\n",
    "    names.append(name)\n",
    "    \n",
    "    result_mean = 0\n",
    "    for result in results:\n",
    "        result_mean += result\n",
    "    result_mean /= len(results)\n",
    "    \n",
    "    print(f'{name}, {result_mean}')\n",
    "    \n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f'{name}: {cv_results.mean()} ({cv_results.std()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc352d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# shape\n",
    "print(dataset.shape)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4568f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# head\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# descriptions\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# class distribution\n",
    "print(dataset.groupby('class').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6672d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can tell from PCA how much variance each is explained by each feature\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(x)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac57e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = pca.transform(x)\n",
    "\n",
    "# number of components\n",
    "n_pcs= pca.components_.shape[0]\n",
    "\n",
    "# get the index of the most important feature on EACH component\n",
    "# LIST COMPREHENSION HERE\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "\n",
    "initial_feature_names = dataset.drop(['class'], axis=1).columns\n",
    "# get the names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "# DICTIONARY COMPREHENSION\n",
    "dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n",
    "\n",
    "# build the dataframe\n",
    "dataset_pca = pd.DataFrame(dic.items(), columns=[\"PCs\", \"Feat.\"])\n",
    "print(dataset_pca.head(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = pca.fit_transform(x)\n",
    "print(x_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def myplot(score,coeff,labels):\n",
    "#     xs = score[:,0]\n",
    "#     ys = score[:,1]\n",
    "#     n = coeff.shape[0]\n",
    "#     scalex = 1.0/(xs.max() - xs.min())\n",
    "#     scaley = 1.0/(ys.max() - ys.min())\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.scatter(xs * scalex,ys * scaley, c = labels, label = labels)\n",
    "#     ax.set_xlim(-1,1)\n",
    "#     ax.set_ylim(-1,1)\n",
    "#     ax.set_xlabel(\"PC{}\".format(1))\n",
    "#     ax.set_ylabel(\"PC{}\".format(2))\n",
    "#     ax.legend()\n",
    "#     ax.grid()\n",
    "\n",
    "# #Call the function. Use only the 2 PCs.\n",
    "# myplot(x_pca[:,0:2],np.transpose(pca.components_[0:2, :]), y)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_features_list = dataset_pca[\"Feat.\"].tolist()[:5]\n",
    "pca_features_list.append(\"class\")\n",
    "# print(pca_features_list)\n",
    "\n",
    "sub_dataset = dataset[pca_features_list]\n",
    "# sub_dataset.head(440)\n",
    "print(sub_dataset['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f7ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "sns.pairplot(sub_dataset, hue=\"class\", height=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ae7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x.values.tolist(), y.values.tolist(), test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr', n_jobs=12)))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier(n_jobs=12)))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto', max_iter=1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59368b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "#     kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "#     cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    print(name, model)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    results.append(accuracy_score(y_test, y_pred))\n",
    "    names.append(name)\n",
    "    \n",
    "    result_mean = 0\n",
    "    for result in results:\n",
    "        result_mean += result\n",
    "    result_mean /= len(results)\n",
    "    \n",
    "    print(f'{name}, {result_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37801be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f'{name}: {cv_results.mean()} ({cv_results.std()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f0164",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_reducer = umap.UMAP(n_jobs=12)\n",
    "tsne_reducer = TSNE(n_jobs=12)\n",
    "\n",
    "umap_embedding = umap_reducer.fit_transform(x)\n",
    "print(\"UMAP Finished\")\n",
    "tsne_embedding = tsne_reducer.fit_transform(x)\n",
    "print(\"TSNE Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53212f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_df = pd.DataFrame(umap_embedding, columns=[\"dim1\", \"dim2\"])\n",
    "# umap_df[\"class\"] = y_original\n",
    "umap_df[\"class\"] = dataset[\"class\"]\n",
    "print(umap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame(tsne_embedding, columns=[\"dim1\", \"dim2\"])\n",
    "# umap_df[\"class\"] = y_original\n",
    "tsne_df[\"class\"] = dataset[\"class\"]\n",
    "print(tsne_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax1 = plt.subplots()\n",
    "# for index, embedding in enumerate(umap_embedding):\n",
    "#     ax1.scatter(\n",
    "#         embedding[0],\n",
    "#         embedding[1],\n",
    "#         c=y.values.tolist()[index],\n",
    "#         label=y.values.tolist()[index],\n",
    "#         s=1)\n",
    "# ax1.set_title(f'UMAP')\n",
    "# fig.show()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(24, 16)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.scatterplot(data=umap_df, x=\"dim1\", y=\"dim2\", hue=\"class\", style=\"class\", \n",
    "                legend=\"full\", palette=sns.color_palette(\"flare\", as_cmap=True),\n",
    "               s=100)\n",
    "# fig.show()\n",
    "fig.savefig(\"umap_8_minute_flows.png\", dpi=300)\n",
    "\n",
    "# fig, ax1 = plt.subplots()\n",
    "# ax1.scatter(\n",
    "#     tsne_embedding[:, 0],\n",
    "#     tsne_embedding[:, 1],\n",
    "#     c=y.values.tolist(),\n",
    "#     label=y.values.tolist(),\n",
    "#     s=1)\n",
    "# ax1.set_title(f'TSNE')\n",
    "# ax1.legend()\n",
    "# fig.show()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(24, 16)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.scatterplot(data=tsne_df, x=\"dim1\", y=\"dim2\", hue=\"class\", style=\"class\", \n",
    "                legend=\"full\", palette=sns.color_palette(\"flare\", as_cmap=True),\n",
    "               s=100)\n",
    "# fig.show()\n",
    "fig.savefig(\"tsne_8_minute_flows.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_umap = umap_df.drop(['class'], axis=1)\n",
    "print(x_umap.head())\n",
    "x_train_umap, x_test_umap, y_train_umap, y_test_umap = train_test_split(x_umap.values.tolist(), y.values.tolist(), test_size=0.30)\n",
    "# Spot Check Algorithms\n",
    "models_umap = []\n",
    "models_umap.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models_umap.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models_umap.append(('KNN', KNeighborsClassifier()))\n",
    "models_umap.append(('CART', DecisionTreeClassifier()))\n",
    "models_umap.append(('NB', GaussianNB()))\n",
    "models_umap.append(('SVM', SVC(gamma='auto')))\n",
    "# evaluate each model in turn\n",
    "results_umap = []\n",
    "names_umap = []\n",
    "for name, model in models:\n",
    "    kfold_umap = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    cv_results_umap = cross_val_score(model, x_train_umap, y_train_umap, cv=kfold_umap, scoring='accuracy')\n",
    "    results_umap.append(cv_results)\n",
    "    names_umap.append(name)\n",
    "    print(f'{name}: {cv_results.mean()} ({cv_results.std()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e9a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
